---
title: Transformers
layout: default
parent: DL Basics
nav_order: 1
---

Transformers have shown strong results on vision and language compared with conventional CNNs and RNNs. Especially on large-scale dataset, the performance of transformers is often significantly better than others.


## Transformers and Self-Attention Mechanism

## Large-Language Models (LLMs)

## Vision Transformers (ViTs)

## Conclusion

----

## Reference
1. [NeurIPS'17] [Attention Is All You Need](https://arxiv.org/abs/1706.03762), Google Research
2. [NeurIPS'20] [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165), OpenAI
3. [arXiv 2020] [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361), JUH and OpenAI
4. [ICLR'20] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929), Google Research
5. [ICCV'21] [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030), Microsoft Research Asia
6. [ECCV'22] [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872), Meta AI
