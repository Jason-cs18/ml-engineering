---
title: Efficient LLMs
layout: default
parent: LLM Systems
nav_order: 2
---
To push the last mile of LLM deplyment, a lot of works focus on training and serving LLMs within low cost. In this blog, we summarize the most important works in efficient LLMs.

## Table of contents
- [Table of contents](#table-of-contents)
- [Background](#background)
- [Survey](#survey)
- [Efficient LLM Serving](#efficient-llm-serving)
- [Efficient LLM Training](#efficient-llm-training)
- [Conclusion](#conclusion)
- [Reference](#reference)

## Background
To catch up the latest progress of efficient LLMs, I highly recommend [the introduction course (TinyML and Efficient Deep Learning Computing)](https://hanlab.mit.edu/courses/2023-fall-65940) from MIT Song Lab.

## Survey
- [arXiv 2023.02] [Full Stack Optimization of Transformer Inference: a Survey](https://arxiv.org/abs/2302.14017), UC Berkeley
- [arXiv 2023.12] [Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/pdf/2312.15234), Carnegie Mellon University
- [TMLR'24] [Efficient Large Language Models: A Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey), The Ohio State University

## Efficient LLM Serving


## Efficient LLM Training


## Conclusion

---

## Reference
1. [Github Repo] [HetServe-LLMs: A Overview of Efficiently Serving Large Language Models across Edge Devices](https://github.com/Jason-cs18/HetServe-LLMs), New York University and Shandong University.
2. [Github Repo] [Awesome LLM Systems Papers](https://github.com/AmberLJC/LLMSys-PaperList), University of Michigan.
