---
title: Transformers
layout: default
parent: DL Basics
nav_order: 1
---
> Date: June 9, 2024 | Estimated Reading Time: 10 min | Author: Yan Lu

Transfromers provide a simple and unified framework to compress information of multiple modals. Intutively, combining them and learning a multimdal representation is feasible and promising for unimodal tasks. In this blog, we summarize the milestone works on multimodal representation learning and discuss their limitations.

Date: June 9, 2024

## Rationale behind multimodal learning
- Data Augmentation

## CLIP

## SAM

## BLIP and BLIP-2

## ImageBind

## LanguageBind

## LLaVA

## Conclusion

----
## References
1. [ICML'21] [Learning Transferable Visual Models From Natural Language Supervision](http://proceedings.mlr.press/v139/radford21a) | OpenAI 
2. [ICCV'23] [Segment Anything](https://arxiv.org/abs/2304.02643) | Meta AI
3. [ICML'22] [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) | Salesforce Research
4. [ICML'23] [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | Salesforce Research
5. [CVPR'23] [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665) | Meta AI
6. [ICLR'24] [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment](https://github.com/PKU-YuanGroup/LanguageBind) | Peking University
7. [NeurIPS'23] [Visual Instruction Tuning](https://llava-vl.github.io/) | University of Wisconsin–Madison
8. [CVPR'24] [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) | University of Wisconsin–Madison
9. [CVPR'23] [Multimodality Helps Unimodality:
Cross-Modal Few-Shot Learning with Multimodal Models](https://linzhiqiu.github.io/papers/cross_modal/) | Carnegie Mellon University