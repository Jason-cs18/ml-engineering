---
title: CLIP and its variants
date: 2023-06-09
layout: default
parent: DL Basics
nav_order: 2
---

Transfromers provide a simple and unified framework to compress information of multiple modals. Intutively, combining them and learning a multimdal representation is feasible and promising for unimodal tasks. In this blog, we summarize the milestone works on multimodal representation learning and discuss their limitations.

## Table of contents
- [Table of contents](#table-of-contents)
- [Rationale behind multimodal learning](#rationale-behind-multimodal-learning)
- [CLIP](#clip)
- [SAM](#sam)
- [BLIP and BLIP-2](#blip-and-blip-2)
- [ImageBind](#imagebind)
- [LanguageBind](#languagebind)
- [LLaVA](#llava)
- [Conclusion](#conclusion)
- [References](#references)

## Rationale behind multimodal learning
- Data Augmentation

## CLIP

## SAM

## BLIP and BLIP-2

## ImageBind

## LanguageBind

## LLaVA

## Conclusion

----
## References
1. [ICML'21] [Learning Transferable Visual Models From Natural Language Supervision](http://proceedings.mlr.press/v139/radford21a)
2. [ICCV'23] [Segment Anything](https://arxiv.org/abs/2304.02643)
3. [ICML'22] [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)
4. [ICML'23] [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
5. [CVPR'23] [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665)
6. [ICLR'24] [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment](https://github.com/PKU-YuanGroup/LanguageBind)
7. [NeurIPS'23] [Visual Instruction Tuning](https://llava-vl.github.io/)
8. [CVPR'24] [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)
9. [CVPR'23] [Multimodality Helps Unimodality:
Cross-Modal Few-Shot Learning with Multimodal Models](https://linzhiqiu.github.io/papers/cross_modal/) 